{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "Search for anomalies in the time series of hotel room prices with unsupervised learning (no labeled data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DATASC~1\\AppData\\Local\\Temp/ipykernel_15048/2182576224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcovariance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEllipticEnvelope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyemma\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmsm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneClassSVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyemma'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from pyemma import msm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pyemma import msm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expedia = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expedia['prop_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = expedia.loc[expedia['prop_id'] == 104517]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "df['price_usd'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['visitor_location_country_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['srch_length_of_stay'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['srch_booking_window'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['srch_room_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['srch_room_count'] == 1]\n",
    "df = df.loc[df['visitor_location_country_id'] == 219]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_time'].min(), df['date_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date_time', 'price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df = df.sort_values('date_time')\n",
    "df['price_usd'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have detected one extreme anomaly which was the Max price_usd at 5584. If an individual data instance can be considered as anomalous with respect to the rest of the data, we call it Point Anomalies (e.g. purchase with large transaction value). We could go back to check the log to see what was it about. After a little bit investigation, I guess it was either a mistake or user seached a presidential suite by accident and had no intention to book or view. In order to find more anomalies that are not extreme, I decided to remove this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expedia.loc[(expedia['price_usd'] == 5584) & (expedia['visitor_location_country_id'] == 219)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['price_usd'] < 5584]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('TimeSeriesExpedia.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_usd'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing that Point Anomaly, we can at least visualize the rest of the data, and perhaps find out more anomalies in several ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "df.plot(x='date_time', y='price_usd', figsize=(12,6))\n",
    "plt.xlabel('Date time')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.title('Time Series of room price by date time of search');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['srch_saturday_night_bool'] == 0, 'price_usd']\n",
    "b = df.loc[df['srch_saturday_night_bool'] == 1, 'price_usd']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(a, bins = 50, alpha=0.5, label='Search Non-Sat Night')\n",
    "plt.hist(b, bins = 50, alpha=0.5, label='Search Sat Night')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Count')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the price is more stable and lower when searching Non-Saturday night. And the price goes up when searching Saturday night. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['srch_saturday_night_bool'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The useful features for our further analysis are \"price_usd\", \"srch_booking_window\" and \"srch_saturday_night_bool\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-Based Anomaly Detection\n",
    "\n",
    "### k-means algorithm\n",
    "\n",
    "k-means is a widely used clustering algorithm. It creates 'k' similar clusters of data points. Data instances that fall outside of these groups could potentially be marked as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them\n",
    "# data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "# min_max_scaler = preprocessing.StandardScaler()\n",
    "# np_scaled = min_max_scaler.fit_transform(data)\n",
    "# data = pd.DataFrame(np_scaled)\n",
    "# reduce to 2 importants features\n",
    "# pca = PCA(n_components=3)\n",
    "# data = pca.fit_transform(data)\n",
    "# standardize these 2 new features\n",
    "# min_max_scaler = preprocessing.StandardScaler()\n",
    "# np_scaled = min_max_scaler.fit_transform(data)\n",
    "# data = pd.DataFrame(np_scaled)\n",
    "\n",
    "# n_cluster = range(1, 20)\n",
    "# kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "# scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.plot(n_cluster, scores)\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Elbow Curve')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above elbow curve, we see that the graph levels off after 10 clusters, implying that addition of more clusters do not explain much more of the variance in our relevant variable; in this case price_usd.\n",
    "\n",
    "we set n_clusters=10, and upon generating the k-means output use the data to plot the 3D clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "X = X.reset_index(drop=True)\n",
    "km = KMeans(n_clusters=10)\n",
    "km.fit(X)\n",
    "km.predict(X)\n",
    "labels = km.labels_\n",
    "#Plotting\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2],\n",
    "          c=labels.astype(np.float), edgecolor=\"k\")\n",
    "ax.set_xlabel(\"price_usd\")\n",
    "ax.set_ylabel(\"srch_booking_window\")\n",
    "ax.set_zlabel(\"srch_saturday_night_bool\")\n",
    "plt.title(\"K Means\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pylab as pl\n",
    "\n",
    "# Y = df[['price_usd']]\n",
    "# X = df[['srch_booking_window']]\n",
    "# Nc = range(1, 20)\n",
    "# kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "# score = [kmeans[i].fit(Y).score(Y) for i in range(len(kmeans))]\n",
    "# plt.figure(figsize=(10,6))\n",
    "# pl.plot(Nc,score)\n",
    "# pl.xlabel('Number of Clusters')\n",
    "# pl.ylabel('Score')\n",
    "# pl.title('Elbow Curve')\n",
    "# pl.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=1).fit(Y)\n",
    "# pca_d = pca.transform(Y)\n",
    "# pca_c = pca.transform(X)\n",
    "# kmeans=KMeans(n_clusters=7)\n",
    "# kmeansoutput=kmeans.fit(Y)\n",
    "# pl.figure('7 Cluster K-Means')\n",
    "# pl.figure(figsize=(10,6))\n",
    "# pl.scatter(pca_c[:, 0], pca_d[:, 0], c=kmeansoutput.labels_)\n",
    "# pl.xlabel('Search booking window')\n",
    "# pl.ylabel('Price USD')\n",
    "# pl.title('7 Cluster K-Means')\n",
    "# pl.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "X = data.values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "#Calculating Eigenvecors and eigenvalues of Covariance matrix\n",
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "# Create a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "# Calculation of Explained Variance from the eigenvalues\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')\n",
    "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first component explains almost 50% of the variance. The second component explains over 30%. However, we've got to notice that almost none of the components are really negligible. The first 2 components contain over  80%  of the information. So, we will set n_components=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them\n",
    "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "data = pd.DataFrame(X_std)\n",
    "# reduce to 2 important features\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit_transform(data)\n",
    "# standardize these 2 new features\n",
    "scaler = StandardScaler()\n",
    "np_scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate with different number of centroids to see the loss plot (elbow method)\n",
    "# n_cluster = range(1, 20)\n",
    "# kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "# scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.plot(n_cluster, scores)\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Score')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "df['cluster'] = kmeans[9].predict(data)\n",
    "df.index = data.index\n",
    "df['principal_feature1'] = data[0]\n",
    "df['principal_feature2'] = data[1]\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Approach\n",
    "\n",
    "The underline assumption in the clustering approach is that if we cluster the data, normal data will belong to clusters while anomalies will not belong to any clusters or belong to small clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the different clusters with the 2 main features\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', 9:'purple', 10:'white', 11: 'grey'}\n",
    "# ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"cluster\"].apply(lambda x: colors[x]))\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return Series of distance between each point and its distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "outliers_fraction = 0.01\n",
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(data, kmeans[9])\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) \n",
    "df['anomaly1'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"anomaly1\"].apply(lambda x: colors[x]))\n",
    "plt.xlabel('principal feature1')\n",
    "plt.ylabel('principal feature2')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.anomaly1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('date_time')\n",
    "df['date_time_int'] = df.date_time.astype(np.int64)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "a = df.loc[df['anomaly1'] == 1, ['date_time_int', 'price_usd']] #anomaly\n",
    "\n",
    "ax.plot(df['date_time_int'], df['price_usd'], color='blue', label='Normal')\n",
    "ax.scatter(a['date_time_int'],a['price_usd'], color='red', label='Anomaly')\n",
    "plt.xlabel('Date Time Integer')\n",
    "plt.ylabel('price in USD')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of anomaly with re-partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = df.loc[df['anomaly1'] == 0, 'price_usd']\n",
    "b = df.loc[df['anomaly1'] == 1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest for anomaly detection.\n",
    "\n",
    "The IsolationForest \"isolates\" observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeable shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "scaler = StandardScaler()\n",
    "np_scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# train isolation forest\n",
    "model =  IsolationForest(contamination=outliers_fraction)\n",
    "model.fit(data)\n",
    "\n",
    "df['anomaly2'] = pd.Series(model.predict(data))\n",
    "# df['anomaly2'] = df['anomaly2'].map( {1: 0, -1: 1} )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "a = df.loc[df['anomaly2'] == -1, ['date_time_int', 'price_usd']] #anomaly\n",
    "\n",
    "ax.plot(df['date_time_int'], df['price_usd'], color='blue', label = 'Normal')\n",
    "ax.scatter(a['date_time_int'],a['price_usd'], color='red', label = 'Anomaly')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anomaly2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of anomaly with avg price repartition\n",
    "a = df.loc[df['anomaly2'] == 1, 'price_usd']\n",
    "b = df.loc[df['anomaly2'] == -1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine-Based Anomaly Detection\n",
    "\n",
    "A support vector machine is another effective technique for detecting anomalies. A SVM is typically associated with supervised learning, but OneClassSVM can be used to identify anomalies as an unsupervised problems.\n",
    "\n",
    "### One class SVM\n",
    "\n",
    "Unsupervised Outlier Detection.\n",
    "\n",
    "Estimate the support of a high-dimensional distribution.\n",
    "\n",
    "The implementation is based on libsvm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
    "scaler = StandardScaler()\n",
    "np_scaled = scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# train oneclassSVM \n",
    "model = OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.01)\n",
    "model.fit(data)\n",
    " \n",
    "df['anomaly3'] = pd.Series(model.predict(data))\n",
    "# df['anomaly3'] = df['anomaly3'].map( {1: 0, -1: 1} )\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "a = df.loc[df['anomaly3'] == -1, ['date_time_int', 'price_usd']] #anomaly\n",
    "\n",
    "ax.plot(df['date_time_int'], df['price_usd'], color='blue', label ='Normal')\n",
    "ax.scatter(a['date_time_int'],a['price_usd'], color='red', label = 'Anomaly')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['anomaly3'] == 1, 'price_usd']\n",
    "b = df.loc[df['anomaly3'] == -1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection using Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class0 = df.loc[df['srch_saturday_night_bool'] == 0, 'price_usd']\n",
    "df_class1 = df.loc[df['srch_saturday_night_bool'] == 1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "df_class0.hist(ax=axs[0], bins=30)\n",
    "df_class1.hist(ax=axs[1], bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class0.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class0 = pd.DataFrame(df_class0)\n",
    "df_class0['deviation'] = envelope.decision_function(X_train)\n",
    "df_class0['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class1.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class1 = pd.DataFrame(df_class1)\n",
    "df_class1['deviation'] = envelope.decision_function(X_train)\n",
    "df_class1['anomaly'] = envelope.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the price repartition by categories with anomalies\n",
    "a0 = df_class0.loc[df_class0['anomaly'] == 1, 'price_usd']\n",
    "b0 = df_class0.loc[df_class0['anomaly'] == -1, 'price_usd']\n",
    "\n",
    "a2 = df_class1.loc[df_class1['anomaly'] == 1, 'price_usd']\n",
    "b2 = df_class1.loc[df_class1['anomaly'] == -1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'])\n",
    "axs[1].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'])\n",
    "axs[0].set_title(\"Search Non Saturday Night\")\n",
    "axs[1].set_title(\"Search Saturday Night\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the data to the main \n",
    "df_class = pd.concat([df_class0, df_class1])\n",
    "df['anomaly5'] = df_class['anomaly']\n",
    "# df['anomaly5'] = np.array(df['anomaly22'] == -1).astype(int)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "a = df.loc[df['anomaly5'] == -1, ('date_time_int', 'price_usd')] #anomaly\n",
    "ax.plot(df['date_time_int'], df['price_usd'], color='blue', label='Normal')\n",
    "ax.scatter(a['date_time_int'],a['price_usd'], color='red', label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['anomaly5'] == 1, 'price_usd']\n",
    "b = df.loc[df['anomaly5'] == -1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain\n",
    "\n",
    "Markov chains can measure the probability of a sequence of events happening. This approach builds a Markov chain for the underline process, and when a sequence of events has happened, we can use the Markov Chain to measure the probability of that sequence occurring and use that to detect any rare sequences.\n",
    "\n",
    "In our price anomaly detection project, we need discretize the data points in define states for markov chain. We will take 'price_usd' to define state for this example and define 5 levels of value (very low, low, average, high, very high)/(VL, L, A, H, VH). Then Markov chains would represent by states VL, L, L, A, A, H, H, VH. And each price would be a price from one state to another state. We can build the Markov chain using historical price data and use the chain to calculate sequence probabilities. Then, we can find the probability of any new sequence happening and then mark rare sequences as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_usd'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train markov model to get transition matrix\n",
    "def getTransitionMatrix (df):\n",
    "    df = np.array(df)\n",
    "    model = msm.estimate_markov_model(df, 1)\n",
    "    return model.transition_matrix\n",
    "\n",
    "# return the success probability of the state change \n",
    "def successProbabilityMetric(state1, state2, transition_matrix):\n",
    "    proba = 0\n",
    "    for k in range(0,len(transition_matrix)):\n",
    "        if (k != (state2-1)):\n",
    "            proba += transition_matrix[state1-1][k]\n",
    "    return 1-proba\n",
    "\n",
    "# return the success probability of the whole sequence\n",
    "def sucessScore(sequence, transition_matrix):\n",
    "    proba = 0 \n",
    "    for i in range(1,len(sequence)):\n",
    "        if(i == 1):\n",
    "            proba = successProbabilityMetric(sequence[i-1], sequence[i], transition_matrix)\n",
    "        else:\n",
    "            proba = proba*successProbabilityMetric(sequence[i-1], sequence[i], transition_matrix)\n",
    "    return proba\n",
    "\n",
    "# return if the sequence is an anomaly considering a threshold\n",
    "def anomalyElement(sequence, threshold, transition_matrix):\n",
    "    if (sucessScore(sequence, transition_matrix) > threshold):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# return a dataframe containing anomaly result for the whole dataset \n",
    "# choosing a sliding windows size (size of sequence to evaluate) and a threshold\n",
    "def markovAnomaly(df, windows_size, threshold):\n",
    "    transition_matrix = getTransitionMatrix(df)\n",
    "    real_threshold = threshold**windows_size\n",
    "    df_anomaly = []\n",
    "    for j in range(0, len(df)):\n",
    "        if (j < windows_size\n",
    "            df_anomaly.append(0)\n",
    "        else:\n",
    "            sequence = df[j-windows_size:j]\n",
    "            sequence = sequence.reset_index(drop=True)\n",
    "            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the different state\n",
    "x1 = (df['price_usd'] <=55).astype(int)\n",
    "x2= ((df['price_usd'] > 55) & (df['price_usd']<=200)).astype(int)\n",
    "x3 = ((df['price_usd'] > 200) & (df['price_usd']<=300)).astype(int)\n",
    "x4 = ((df['price_usd'] > 300) & (df['price_usd']<=400)).astype(int)\n",
    "x5 = (df['price_usd'] >400).astype(int)\n",
    "df_mm = x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5\n",
    "\n",
    "# getting the anomaly labels for our dataset (evaluating sequence of 5 values and anomaly = less than 20% probable)\n",
    "df_anomaly = markovAnomaly(df_mm, 5, 0.20)\n",
    "df_anomaly = pd.Series(df_anomaly)\n",
    "print(df_anomaly.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anomaly24'] = df_anomaly\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "a = df.loc[df['anomaly24'] == 1, ('date_time_int', 'price_usd')] #anomaly\n",
    "\n",
    "ax.plot(df['date_time_int'], df['price_usd'], color='blue')\n",
    "ax.scatter(a['date_time_int'],a['price_usd'], color='red')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[df['anomaly24'] == 0, 'price_usd']\n",
    "b = df.loc[df['anomaly24'] == 1, 'price_usd']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(16,6))\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our anomaly detection is unsupervised learning.  After building the models, we have no idea how well it is doing as we have nothing to test it against. Hence, the results of those methods need to be tested in the field before placing them in the critical path."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
