{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest is one of the most used techniques to detect anomalies in the data. It’s based on a “forest” of trees, where each isolation tree isolates anomalous observations from the rest of the data points. Despite its simplicity, speed and intuitiveness, there is a drawback. The lack of explanation. Why is a particular observation considered anomalous by the algorithm? How can the output be interpreted?\n",
    "There are two possible interpretations, Global and Local. Global because the goal is to explain the model as a whole and understand what features have a more relevant role in the general model. On the other hand, we need to look at the model locally to have an idea of the features that influence a specific prediction of the model. The features can vary passing from a local to a global explanation and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will show how to comprehend the decisions taken by Isolation Forest in detecting anomalies in the data. This is possible using the data visualizations provided by SHAP. For the global interpretation, you’ll see the summary plot and the global bar plot, while for local interpretation two most used graphs are the force plot, the waterfall plot and the scatter/dependence plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Isolation Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(data=boston.data,columns=boston.feature_names)\n",
    "df['target']=boston.target\n",
    "df.head()\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 506 neighbourhood regions around Boston. The typical task is to predict the median value of owner-occupied homes, but in this case, we want to train the isolation forest to divide the dataset into normal and anomalous observations. There are 13 features and the target variable in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not originally designed for an anomaly detection task, but let’s suppose that there are anomalous data entries. For example, in the list, we could have houses with very low prices in the centre of Boston, a little too cheap with respect to the buildings in the same area. Then, the goal is to distinguish these anomalous cases from the normal ones.\n",
    "For this anomaly detection problem, we’ll use the Isolation forest. There are some advantages to use this model. First, it’s unsupervised and we don’t need target labels (that we don’t have) to train the model. Second, there are two useful properties considered to detect anomalies:\n",
    "The anomalies are few in numbers, so it constitutes a minority class\n",
    "The anomalous observations are characterized to have very different values of features from the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that the isolation procedure should isolate fastly the outliers at small depths, while the normal instances are usually at the bottom of the tree. For all these properties, the Isolation tree was taken into consideration in the Boston housing dataset.\n",
    "Let’s create the model using the IsolationForest function from sklearn library. We’ll fix the random state equal to 42 to reproduce the same output every time. The proportion of anomalies in our dataset is specified by the argument “contamination” and will be automatically determined. The typical value is 0.1 and usually depends on the dimension of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "      <th>anomaly_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.01360</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410</td>\n",
       "      <td>5.888</td>\n",
       "      <td>47.6</td>\n",
       "      <td>7.3197</td>\n",
       "      <td>3.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.80</td>\n",
       "      <td>18.9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.01432</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411</td>\n",
       "      <td>6.816</td>\n",
       "      <td>40.5</td>\n",
       "      <td>8.3248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>392.90</td>\n",
       "      <td>3.95</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.62864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624</td>\n",
       "      <td>5.019</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4394</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>34.41</td>\n",
       "      <td>14.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3.32105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.871</td>\n",
       "      <td>5.403</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3216</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>26.82</td>\n",
       "      <td>13.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM     ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "8    0.21124   12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
       "54   0.01360   75.0   4.00   0.0  0.410  5.888   47.6  7.3197  3.0  469.0   \n",
       "57   0.01432  100.0   1.32   0.0  0.411  6.816   40.5  8.3248  5.0  256.0   \n",
       "141  1.62864    0.0  21.89   0.0  0.624  5.019  100.0  1.4394  4.0  437.0   \n",
       "142  3.32105    0.0  19.58   1.0  0.871  5.403  100.0  1.3216  5.0  403.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  anomaly_label  \n",
       "8       15.2  386.63  29.93    16.5             -1  \n",
       "54      21.1  396.90  14.80    18.9             -1  \n",
       "57      15.1  392.90   3.95    31.6             -1  \n",
       "141     21.2  396.90  34.41    14.4             -1  \n",
       "142     14.7  396.90  26.82    13.4             -1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iforest = IsolationForest(n_estimators=100, max_samples='auto', \n",
    "                          contamination='auto', max_features=13, \n",
    "                          bootstrap=False, n_jobs=-1, random_state=42)\n",
    "\n",
    "iforest.fit(X)\n",
    "\n",
    "y_pred = iforest.predict(X)\n",
    "\n",
    "df['anomaly_label']=y_pred\n",
    "print(df.anomaly_label.unique())\n",
    "df[df.anomaly_label==-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=df['anomaly_label'].apply(lambda x: 1 if x==-1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we’ll use all the dataset because we don’t have prior knowledge of the anomalous observations. So, it’s an unsupervised problem. Once the model is trained, we can make the predictions and save them in a column, called anomaly_label. It assigns -1 to the anomalous observations and 1 to the inliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute SHAP values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP provides implementations of explanations that leverage Shapley values. TreeExplainer for models based on trees, DeepExplainer and GradientExplainer for neural networks. Last, KernelExplainer and Explainer are applied for any type of model.\n",
    "Even if it’s a model based on trees, the TreeExplainer doesn’t work well with Isolation Forest. So, I will apply the Explainer function.\n",
    "Now, I will show the magical rows of code that will make you discover the secrets of the model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutation explainer: 507it [11:46,  1.41s/it]                         \n"
     ]
    }
   ],
   "source": [
    "explainer = shap.Explainer(iforest.predict, X)\n",
    "shap_values = explainer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we defined the explainer object, using the function Explainer that performs the isolation forest by taking the prediction method. Once this object is created, you can obtain the Shapley values. Before beginning to display the SHAP’s plot, you need to load the Javascript library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "matplotlib is not installed so plotting is not available! Run `pip install matplotlib` to fix this.\n"
     ]
    }
   ],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explain Single Prediction\n",
    "We can start by explaining single predictions of our model. A clear way to observe the contribution of each feature in the model’s outcome is to display the **force plot**. I will show the outcome of the 142nd item, which was previously classified as an anomaly by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id='i3R3LDE65ZKBO5BT9R75G'>\n",
       "<div style='color: #900; text-align: center;'>\n",
       "  <b>Visualization omitted, Javascript library not loaded!</b><br>\n",
       "  Have you run `initjs()` in this notebook? If this notebook was from another\n",
       "  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n",
       "  this notebook on github the Javascript has been stripped for security. If you are using\n",
       "  JupyterLab this error is because a JupyterLab extension has not yet been written.\n",
       "</div></div>\n",
       " <script>\n",
       "   if (window.SHAP) SHAP.ReactDom.render(\n",
       "    SHAP.React.createElement(SHAP.AdditiveForceVisualizer, {\"outNames\": [\"f(x)\"], \"baseValue\": 0.64, \"outValue\": -0.9999999999999999, \"link\": \"identity\", \"featureNames\": [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"], \"features\": {\"0\": {\"effect\": -0.13055555555555556, \"value\": 3.32105}, \"1\": {\"effect\": 0.06111111111111113, \"value\": 0.0}, \"2\": {\"effect\": -0.05000000000000001, \"value\": 19.58}, \"3\": {\"effect\": -0.691111111111111, \"value\": 1.0}, \"4\": {\"effect\": -0.3788888888888889, \"value\": 0.871}, \"5\": {\"effect\": -0.044444444444444446, \"value\": 5.403}, \"6\": {\"effect\": -0.019444444444444448, \"value\": 100.0}, \"7\": {\"effect\": -0.06111111111111113, \"value\": 1.3216}, \"8\": {\"effect\": -0.020555555555555553, \"value\": 5.0}, \"9\": {\"effect\": -0.019444444444444445, \"value\": 403.0}, \"10\": {\"effect\": -0.12944444444444445, \"value\": 14.7}, \"11\": {\"effect\": 0.03277777777777778, \"value\": 396.9}, \"12\": {\"effect\": -0.18888888888888886, \"value\": 26.82}}, \"plot_cmap\": \"RdBu\", \"labelMargin\": 20}),\n",
       "    document.getElementById('i3R3LDE65ZKBO5BT9R75G')\n",
       "  );\n",
       "</script>"
      ],
      "text/plain": [
       "<shap.plots._force.AdditiveForceVisualizer at 0x14ea63c73d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap.plots.force(shap_values[142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8931ca67e55eb2504d350e8273275826e7b772a1dc834a27a0a91cd32fb53525"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
