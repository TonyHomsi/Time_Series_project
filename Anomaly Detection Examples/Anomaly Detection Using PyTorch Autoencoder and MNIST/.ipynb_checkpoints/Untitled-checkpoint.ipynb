{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce72fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "                       \n",
    "import pandas as pd                       \n",
    "import matplotlib.pyplot as plt                       \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b793e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf6ee7",
   "metadata": {},
   "source": [
    "We use the test file datapoints since they will be excluded from training and therefore our model will not see any of these. Using the test datapoints we will select a subset for us to corrupt. Let’s arbitrarily use the first 1000 (index 0–999) for corruption and leave the remaining 9000 datapoints untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356c8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "anom = df[:1000]\n",
    "clean = df[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d50b945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f908eca",
   "metadata": {},
   "source": [
    "We will store the first 1000 rows of mnist_test.csv as our anomalies (don’t worry, later we will shuffle it all up) in a separate DataFrame and join them later.\n",
    "Next we want to corrupt (add excessive noise) to these 1000 datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06d0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(anom)):\n",
    "    # select row from anom\n",
    "    row = anom.iloc[i]\n",
    "    # iterate through each element in row\n",
    "    for i in range(len(row)-1):\n",
    "        # add noise to element\n",
    "        row[i+1] = min(255, row[i+1]+random.randint(100,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06ab34",
   "metadata": {},
   "source": [
    "This effectively adds a randThis effectively adds a random amount of noise to each pixel of a MNIST datapoint. It is fairly excessive, but it can be an interesting experiment by changing the level of noise to see how our model reacts. This change can be reflected in ‘randint(lower, upper)’ by giving ‘lower’ 0 and ‘upper’ 255 values. For this article we will use very corrupted data.\n",
    "\n",
    "Here is what first first five rows of the ‘anom’ DataFrame now (bottom row) looks like compared to before (top row):om amount of noise to each pixel of a MNIST datapoint. It is fairly excessive, but it can be an interesting experiment by changing the level of noise to see how our model reacts. This change can be reflected in ‘randint(lower, upper)’ by giving ‘lower’ 0 and ‘upper’ 255 values. For this article we will use very corrupted data.\n",
    "Here is what first first five rows of the ‘anom’ DataFrame now (bottom row) looks like compared to before (top row):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b279f3",
   "metadata": {},
   "source": [
    "Not only are we adding noise, we will also edit the label to a binary annotation: anomalous or non-anomalous, which will be 1 and 0 respectively. We will use this label in the final stage to determine how many anomalies we successfully identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c513ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DATASC~1\\AppData\\Local\\Temp/ipykernel_24624/2944837865.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anom['label'] = 1\n",
      "C:\\Users\\DATASC~1\\AppData\\Local\\Temp/ipykernel_24624/2944837865.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean['label'] = 0\n"
     ]
    }
   ],
   "source": [
    "anom['label'] = 1\n",
    "clean['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d4353",
   "metadata": {},
   "source": [
    "All that is left is to join up these two DataFrames, shuffle and save it to its own file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cdf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_test = pd.concat([anom, clean])  # join\n",
    "an_test.sample(frac=1)              # shuffle\n",
    "an_test.to_csv('anom.csv')          # save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d022a2",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "The neural network of choice for our anomaly detection application is the Autoencoder. This is due to the autoencoders ability to perform feature extraction as the dimensionality is reduced to build a latent representation of the input distribution. How we can exploit that is by utilizing a loss distribution of rebuilt inputs to outputs (which turns out to be Guassian) and making the assumption that any outliers will be anomalies since they faulter well outside the parameters of what the model considers “within the expected distribution”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b093c5",
   "metadata": {},
   "source": [
    "For our purposes the following architecture was used as a simple linear compression from input to latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3bab7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encode = self.enc(x)\n",
    "        decode = self.dec(encode)\n",
    "        return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8665c",
   "metadata": {},
   "source": [
    "## Training and Prediction Setup\n",
    "Libraries required for training and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfc73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6cb7d4",
   "metadata": {},
   "source": [
    "Core training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddcb405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-2         # learning rate\n",
    "w_d = 1e-5        # weight decay\n",
    "momentum = 0.9   \n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c5729",
   "metadata": {},
   "source": [
    "Training\n",
    "Using the model mentioned in the previous section, we will now train on the standard MNIST training dataset (our mnist_train.csv file). Since we’re using the CSV file, we will implement a custom dataset loader with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6ddd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        super(Loader, self).__init__()\n",
    "        self.dataset = ''\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        row = row.drop(labels={'label'})\n",
    "        data = torch.from_numpy(np.array(row)/255).float()\n",
    "        return data\n",
    "    \n",
    "class Train_Loader(Loader):\n",
    "    def __init__(self):\n",
    "        super(Train_Loader, self).__init__()\n",
    "        self.dataset = pd.read_csv(\n",
    "                       'mnist_train.csv',\n",
    "                       index_col=False\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cbf36",
   "metadata": {},
   "source": [
    "This custom dataset loader removes the label column of each row and normalizes (divides by 255) to a 0–1 range that better serves training efficiency. The ‘Train_Loader’ implements the base class ‘Loader’. It was built this way as a ‘Test_Loader’ class can easily be implemented using the same base class.\n",
    "\n",
    "In order to enumerate over the dataset during training we extend to the PyTorch DataLoader class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff474962",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Train_Loader()\n",
    "train_ = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42fd80d",
   "metadata": {},
   "source": [
    "The training setup includes a dictionary of lists named metrics — this is a personal favorite if I have to track multiple values throughout training. The rest of the parameters are pretty standard. As mentioned before, we will be implemented the MSELoss class as our loss function between output and input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29aedb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AE()\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=w_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe161c",
   "metadata": {},
   "source": [
    "Now we can train our model with the following loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    ep_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    for bx, (data) in enumerate(train_):\n",
    "        sample = model(data.to(device))\n",
    "        loss = criterion(data.to(device), sample)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss/len(train_set)\n",
    "    metrics['train_loss'].append(epoch_loss)\n",
    "    ep_end = time.time()\n",
    "    print('-----------------------------------------------')\n",
    "    print('[EPOCH] {}/{}\\n[LOSS] {}'.format(epoch+1,epochs,epoch_loss))\n",
    "    print('Epoch Complete in {}'.format(timedelta(seconds=ep_end-ep_start)))\n",
    "end = time.time()\n",
    "print('-----------------------------------------------')\n",
    "print('[System Complete: {}]'.format(timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f14ff",
   "metadata": {},
   "source": [
    "Once training is finished, we output the loss plot to determine if our model has converged to a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(15,10))\n",
    "ax.set_title('Loss')\n",
    "ax.plot(metrics['train_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e666ab",
   "metadata": {},
   "source": [
    "Excellent! It looks like we’ve managed to converge to a solution: the Autoencoder has successfully captured the features of the input distribution within its compressed latent representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05db675",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "For our model to determine if an input is or is not an anomaly, we will use the loss value from the output and input — if the loss value is high, then we will assume that the model is seeing an element that is outside of the known distribution representation. To achieve this, we will iterate through our test set sequentially and retaining the loss value. It is very important to perform this task sequentially as this will serve us in our analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss_dist = []\n",
    "anom = pd.read_csv('anom.csv', index_col=[0])\n",
    "#for bx, data in enumerate(test_):\n",
    "for i in range(len(anom)):\n",
    "    data = torch.from_numpy(np.array(anom.iloc[i][1:])/255).float()\n",
    "    sample = model(data.to(device))\n",
    "    loss = criterion(data.to(device), sample)\n",
    "    loss_dist.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbef1aa",
   "metadata": {},
   "source": [
    "## Results\n",
    "Visualizing the loss values will give us valuable insight to where our anomalies are hiding. A simple way of doing this is by projecting each value as a point and observing the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sc = []\n",
    "for i in loss_dist:\n",
    "    loss_sc.append((i,i))\n",
    "plt.scatter(*zip(*loss_sc))\n",
    "plt.axvline(0.3, 0.0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae49512",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_threshold = 0.0\n",
    "upper_threshold = 0.3\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Loss Distribution')\n",
    "sns.distplot(loss_dist,bins=100,kde=True, color='blue')\n",
    "plt.axvline(upper_threshold, 0.0, 10, color='r')\n",
    "plt.axvline(lower_threshold, 0.0, 10, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e8f0e",
   "metadata": {},
   "source": [
    "In both the above plots of the loss values, you noticed a vertical line. These lines are an estimated threshold value for which we will determine a loss value is or is not an anomaly. In the loss distribution plot, if the value exceed (to the right) of the red line, we will consider that data as an anomaly. There is a blue line that represents a lower threshold (anything below) but is not relevant for this example of data.\n",
    "\n",
    "Using this upper threshold, we can make predictions on what we consider an anomaly and count the number of occurences as follows:\n",
    "\n",
    "* TP (True Positive): Both the prediction and label align for anomaly\n",
    "* FP (False Positive): The prediction determines anomaly but the label is non-anomalout\n",
    "* TN (True Negative): Both the prediction and label align for non-anomalous\n",
    "* FN (False Negative): The prediction determines non-anomaly but the label is anomalous\n",
    "\n",
    "The following code is why it was so important to retain the sequential ordering of our loss values. We match up the loss values to each row within the MNIST anomaly test set we have created. If this ordering was altered then we would be associating the wrong loss value with the wrong input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anom.csv', index_col=[0])\n",
    "ddf = pd.DataFrame(columns=df.columns)\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_anom = 0\n",
    "for i in range(len(loss_dist)):\n",
    "    total_anom += df.iloc[i]['label']\n",
    "    if loss_dist[i] >= upper_threshold:\n",
    "        n_df = pd.DataFrame([df.iloc[i]])\n",
    "        n_df['loss'] = loss_dist[i]\n",
    "        ddf = pd.concat([df,n_df], sort = True)\n",
    "        if float(df.iloc[i]['label']) == 1.0:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if float(df.iloc[i]['label']) == 1.0:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "print('[TP] {}\\t[FP] {}\\t[MISSED] {}'.format(tp, fp, total_anom-tp))\n",
    "print('[TN] {}\\t[FN] {}'.format(tn, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e17bf",
   "metadata": {},
   "source": [
    "Placing our threshold at 0.3 gives us a 100% success rate for predicting anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1cba2",
   "metadata": {},
   "source": [
    "This information can be best visualized as a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc05475",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = [[tn,fp],[fn,tp]]\n",
    "plt.figure()\n",
    "sns.heatmap(conf,annot=True,annot_kws={\"size\": 16},fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80bdb8",
   "metadata": {},
   "source": [
    "Using a traditional autoencoder built with PyTorch, we can identify 100% of aomalies. The framework can be copied and run in a Jupyter Notebook with ease. Test yourself and challenge the thresholds of identifying different kinds of anomalies! This can be extended to other use-cases with little effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f27179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
